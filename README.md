# RAG-Based-AI-Assistant
A Retrieval-Augmented Generation (RAG) app using LangChain, ChromaDB, Sentence Transformers, and Groqâ€™s LLM to answer questions strictly from your PDF or text documentsâ€”ensuring factual, document-grounded responses with zero hallucination.

## Project Structure
```
RAG-Based-AI-Assistant/
â”‚
â”œâ”€â”€ .venv/                        # Virtual environment (not pushed to GitHub)
â”‚
â”œâ”€â”€ data/                         # Folder for source documents
â”‚   â”œâ”€â”€ ai.txt
â”‚   â”œâ”€â”€ climate_change.txt
â”‚   â””â”€â”€ Space exploration.txt
â”‚
â”œâ”€â”€ src/                          # Main source code folder
â”‚   â”œâ”€â”€ app.py                    # Main application (entry point)
â”‚   â””â”€â”€ vectorDB.py               # Handles ChromaDB setup and embeddings
â”‚
â”œâ”€â”€ .env                          # Stores environment variables like GROQ_API_KEY
â”‚
â”œâ”€â”€ .gitignore                    # Prevents .venv and .env from being tracked
â”‚
â”œâ”€â”€ LICENSE                       # License file
â”‚
â””â”€â”€ requirements.txt              # Python dependencies
```

## Setup Instructions (VS Code Terminal)

1ï¸âƒ£ Clone the repository
git clone https://github.com/raneAshutoshDs21/RAG-Based-AI-Assistant.git
cd RAG-Based-AI-Assistant

2ï¸âƒ£ Create a virtual environment
python -m venv .venv

3ï¸âƒ£ Activate the virtual environment

Windows PowerShell

.venv\Scripts\activate


macOS/Linux

source .venv/bin/activate

4ï¸âƒ£ Upgrade pip (optional but recommended)
python -m pip install --upgrade pip

5ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ”‘ Environment Variables

Create a .env file inside your project root and add:

GROQ_API_KEY=your_groq_api_key_here


âš ï¸ Make sure .env is included in .gitignore (to keep your API key private).

ğŸš€ Running the Project

Run the app directly from the terminal inside VS Code:

python src/app.py


If youâ€™re using Streamlit UI, then run:

streamlit run src/app.py

ğŸ§© How It Works

Document Loading:
All .txt or .pdf files from the data/ folder are read.

Chunking & Embedding:
Each document is split into smaller chunks and embedded using the SentenceTransformer model (all-MiniLM-L6-v2).

Vector Database (ChromaDB):
The embeddings are stored locally inside a Chroma collection called rag_documents.

Retrieval + Generation:
When a user asks a question, the app:

Retrieves the most relevant chunks from ChromaDB.

Passes them to the Groq LLM for context-aware answers.

Ensures the model responds strictly based on the loaded documents.

ğŸ§° Tech Stack
Component	Purpose
LangChain	Framework for chaining LLM and retriever
ChromaDB	Local vector store for document embeddings
SentenceTransformers	Generates semantic embeddings for documents
Groq API	Provides fast, low-latency LLM inference
Python 3.10+	Core programming language
dotenv	Secure environment variable management
ğŸ§  Example Query

After running the app:

python src/app.py


Example conversation:

ğŸ§‘â€ğŸ’» Ask a question (or type 'quit' to exit): What is artificial intelligence?

ğŸ¤– Answer:

## Project Working

<img width="1538" height="789" alt="Screenshot 2025-11-11 191522" src="https://github.com/user-attachments/assets/c2f18d5a-5b15-477e-9646-1946196e8c21" />



ğŸ§¾ License

This project is licensed under the MIT License
.
